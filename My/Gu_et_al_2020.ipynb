{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 지정된 경로를 찾을 수 없습니다: '/content/drive/MyDrive/Replicate/Gu et al.(2020)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\INHA\\Desktop\\Empirical_Asset_Pricing_via_Machine_Learning\\My\\Gu_et_al_2020.ipynb Cell 1\u001b[0m line \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/INHA/Desktop/Empirical_Asset_Pricing_via_Machine_Learning/My/Gu_et_al_2020.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcross_decomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m PLSRegression\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/INHA/Desktop/Empirical_Asset_Pricing_via_Machine_Learning/My/Gu_et_al_2020.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m PCA\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/INHA/Desktop/Empirical_Asset_Pricing_via_Machine_Learning/My/Gu_et_al_2020.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m os\u001b[39m.\u001b[39;49mchdir(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/content/drive/MyDrive/Replicate/Gu et al.(2020)\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/INHA/Desktop/Empirical_Asset_Pricing_via_Machine_Learning/My/Gu_et_al_2020.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/INHA/Desktop/Empirical_Asset_Pricing_via_Machine_Learning/My/Gu_et_al_2020.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 지정된 경로를 찾을 수 없습니다: '/content/drive/MyDrive/Replicate/Gu et al.(2020)'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor, ElasticNet\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "os.chdir(r'/content/drive/MyDrive/Replicate/Gu et al.(2020)')\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data_ch, data_ma, stdt, nddt):\n",
    "    ## firm characteristics\n",
    "    data_ch = data_ch[(data_ch['DATE']>=stdt)&(data_ch['DATE']<=nddt)].reset_index(drop=True)\n",
    "    data_ch['DATE'] = pd.to_datetime(data_ch['DATE'],format='%Y%m%d')+pd.offsets.MonthEnd(0)\n",
    "    characteristics = list(set(data_ch.columns).difference({'permno','DATE','SHROUT','mve0','sic2','RET','prc'}))\n",
    "    data_ch = fill_na(data_ch, characteristics)\n",
    "    data_ch = XS_MinMax(data_ch, characteristics)\n",
    "    data_ch = get_sic_dummies(data_ch)\n",
    "\n",
    "    ## macroeconomic predictors\n",
    "    data_ma = data_ma[(data_ma['yyyymm']>=stdt//100)&(data_ma['yyyymm']<=nddt//100)].reset_index(drop=True)\n",
    "    ma_predictors = ['dp_sp','ep_sp','bm_sp','ntis','tbl','tms','dfy','svar']\n",
    "    data_ma['Index'] = data_ma['Index'].str.replace(',','').astype('float64')\n",
    "    data_ma['dp_sp'] = np.log(data_ma['D12'])-np.log(data_ma['Index'])\n",
    "    data_ma['ep_sp'] = np.log(data_ma['E12'])-np.log(data_ma['Index'])\n",
    "    data_ma.rename({'b/m':'bm_sp'},axis=1,inplace=True)\n",
    "    data_ma['tms'] = data_ma['lty']-data_ma['tbl']\n",
    "    data_ma['dfy'] = data_ma['BAA']-data_ma['AAA']\n",
    "    data_ma = data_ma[['yyyymm']+ma_predictors]\n",
    "    data_ma['yyyymm'] = pd.to_datetime(data_ma['yyyymm'],format='%Y%m')+pd.offsets.MonthEnd(0)\n",
    "\n",
    "    ## merge and add interaction variables\n",
    "    data = interactions(data_ch, data_ma, characteristics, ma_predictors)\n",
    "    data = data.set_index('DATE')\n",
    "    data = data.sort_values(['DATE','permno'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing data\n",
    "def fill_na(data_ch, characteristics):\n",
    "    for ch in characteristics:\n",
    "         data_ch[ch] = data_ch.groupby('DATE')[ch].transform(lambda x: x.fillna(x.median()))\n",
    "    for ch in characteristics:\n",
    "         data_ch[ch] = data_ch[ch].fillna(0)\n",
    "    return data_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-sectional transformation to [-1,1]\n",
    "def XS_MinMax(data_ch, characteristics):\n",
    "    for ch in characteristics:\n",
    "         data_ch[ch] = data_ch.groupby('DATE')[ch].transform(lambda x: minmax_scale(np.array(x).reshape((len(x),1)),(-1,1)).flatten())\n",
    "    return data_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummy variables for sic code\n",
    "def get_sic_dummies(data_ch):\n",
    "    sic_dummies = pd.get_dummies(data_ch['sic2'].fillna(999).astype(int),prefix='sic').drop('sic_999',axis=1)\n",
    "    data_ch_d = pd.concat([data_ch,sic_dummies],axis=1)\n",
    "    data_ch_d.drop(['prc','SHROUT','mve0','sic2'],inplace=True,axis=1)\n",
    "    return data_ch_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge and add interaction variables\n",
    "def interactions(data_ch, data_ma, characteristics, ma_predictors):\n",
    "    # construct interactions between firm characteristics and macroeconomic predictors\n",
    "    data = data_ch.copy()\n",
    "    data_ma_long = pd.merge(data[['DATE']],data_ma,left_on='DATE',right_on='yyyymm',how='left')\n",
    "    data = data.reset_index(drop=True)\n",
    "    data_ma_long = data_ma_long.reset_index(drop=True)\n",
    "    for fc in characteristics:\n",
    "        for mp in ma_predictors:\n",
    "            data[fc+'*'+mp] = data[fc]*data_ma_long[mp]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## validation function to find the best model\n",
    "def val_fun(model, params: dict, X_trn, y_trn, X_vld, y_vld):\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    for param in lst_params:\n",
    "        if best_ros == None:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            best_mod = mod\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            best_ros = R_oos(y_vld, y_pred)\n",
    "            best_param = param\n",
    "        else:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            ros = R_oos(y_vld, y_pred)\n",
    "            if ros > best_ros:\n",
    "                best_ros = ros\n",
    "                best_mod = mod\n",
    "                best_param = param\n",
    "    return best_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_oos(actual, predicted):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    #predicted = np.where(predicted<0,0,predicted)\n",
    "    return 1 - (np.dot((actual-predicted),(actual-predicted)))/(np.dot(actual,actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pred(file_path,res,res_top,res_bot):\n",
    "    if not os.path.exists(file_path+'prediction'):\n",
    "        os.mkdir(file_path+'prediction')\n",
    "    if not os.path.exists(file_path+'results'):\n",
    "        os.mkdir(file_path+'results')\n",
    "\n",
    "    res.to_csv(file_path+'prediction/res.csv')\n",
    "    res_top.to_csv(file_path+'prediction/res_top.csv')\n",
    "    res_bot.to_csv(file_path+'prediction/res_bot.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLS with expanding window\n",
    "def EXP_OLS(data,fts,tgt,yrs_trn):\n",
    "    X = data[fts]\n",
    "    y = data[[tgt]]\n",
    "    y_pred = []\n",
    "    R2_oos = []\n",
    "\n",
    "    # deal with dates\n",
    "    all_dates = pd.Series(data.index).drop_duplicates().sort_values().reset_index(drop=True)\n",
    "\n",
    "    # train test split and prediction\n",
    "    for i in np.arange(12*yrs_trn-1,len(all_dates[:-12]),12):\n",
    "        nddt_trn = all_dates[i]\n",
    "        nddt_tst = all_dates[i+12]\n",
    "        X_trn, y_trn = X[X.index<=nddt_trn], y[y.index<=nddt_trn]\n",
    "        X_tst, y_tst= X[(X.index>nddt_trn)&(X.index<=nddt_tst)], y[(y.index>nddt_trn)&(y.index<=nddt_tst)]\n",
    "        mod = LinearRegression().fit(X_trn,y_trn)\n",
    "        y_pred = list(mod.predict(X_tst))\n",
    "        R2_oos.append(R_oos(y_tst, mod.predict(X_tst)))\n",
    "\n",
    "    return y_pred, R2_oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Huber regression with expanding window\n",
    "def EXP_HUBER(data,fts,tgt,yrs_trn,epsilon):\n",
    "    X = data[fts]\n",
    "    y = data[[tgt]]\n",
    "    y_pred = []\n",
    "\n",
    "    # deal with dates\n",
    "    all_dates = pd.Series(data.index).drop_duplicates().sort_values().reset_index(drop=True)\n",
    "\n",
    "    # train test split and prediction\n",
    "    for i in np.arange(12*yrs_trn-1,len(all_dates[:-12]),12):\n",
    "        nddt_trn = all_dates[i]\n",
    "        nddt_tst = all_dates[i+12]\n",
    "        X_trn, y_trn = X[X.index<=nddt_trn], y[y.index<=nddt_trn]\n",
    "        X_tst, y_tst= X[(X.index>nddt_trn)&(X.index<=nddt_tst)], y[(y.index>nddt_trn)&(y.index<=nddt_tst)]\n",
    "        mod = HuberRegressor(epsilon=epsilon).fit(X_trn,y_trn)\n",
    "        y_pred += list(mod.predict(X_tst).flatten())\n",
    "\n",
    "    R2_oos = R_oos(y_tst ,y_pred)\n",
    "    return R2_oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLS with expanding window\n",
    "def EXP_PLS(data,fts,tgt,yrs_trn,yrs_vld,params):\n",
    "    X = data[fts]\n",
    "    y = data[[tgt]]\n",
    "    y_pred = []\n",
    "\n",
    "    # deal with dates\n",
    "    all_dates = pd.Series(data.index).drop_duplicates().sort_values().reset_index(drop=True)\n",
    "\n",
    "    # train validate test split and prediction\n",
    "    for i in np.arange(12*yrs_trn-1,len(all_dates[:-12*(yrs_vld+1)]),12):\n",
    "        nddt_trn = all_dates[i]\n",
    "        nddt_vld = all_dates[i+yrs_vld*12]\n",
    "        nddt_tst = all_dates[i+(yrs_vld+1)*12]\n",
    "        X_trn, y_trn = X[X.index<=nddt_trn], y[y.index<=nddt_trn]\n",
    "        X_vld, y_vld = X[(X.index>nddt_trn)&(X.index<=nddt_vld)], y[(y.index>nddt_trn)&(y.index<=nddt_vld)]\n",
    "        X_tst, y_tst = X[(X.index>nddt_vld)&(X.index<=nddt_tst)], y[(y.index>nddt_vld)&(y.index<=nddt_tst)]\n",
    "        mod = val_fun(PLSRegression, params, X_trn, y_trn, X_vld, y_vld)\n",
    "        y_pred += list(mod.predict(X_tst).flatten())\n",
    "\n",
    "    R2_oos = R_oos(y_tst ,y_pred)\n",
    "    return R2_oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCR class\n",
    "class PCRegressor:\n",
    "\n",
    "    def __init__(self,n_PCs=1,loss='mse'):\n",
    "        self.n_PCs = n_PCs\n",
    "        if loss not in ['huber','mse']:\n",
    "            raise AttributeError(\n",
    "            f\"The loss should be either 'huber' or 'mse', but {loss} is given\"\n",
    "            )\n",
    "        else:\n",
    "            self.loss = loss\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        X = np.array(X)\n",
    "        N,K = X.shape\n",
    "        y = np.array(y).flatten()\n",
    "        self.mu = np.mean(X,axis=0).reshape((1,K))\n",
    "        self.sigma = np.std(X,axis=0).reshape((1,K))\n",
    "        self.sigma = np.where(self.sigma==0,1,self.sigma)\n",
    "        X = (X-self.mu)/self.sigma\n",
    "        pca = PCA()\n",
    "        X = pca.fit_transform(X)[:,:self.n_PCs]\n",
    "        self.Vk = pca.components_.T[:,:self.n_PCs]\n",
    "        if self.loss == 'mse':\n",
    "            self.model = LinearRegression().fit(X,y)\n",
    "        else:\n",
    "            self.model = HuberRegressor().fit(X,y)\n",
    "        self.beta = self.Vk.T @ self.model.coef_\n",
    "        return self\n",
    "\n",
    "    def predict(self,X):\n",
    "        X = np.array(X)\n",
    "        X = (X-self.mu)/self.sigma\n",
    "        pred = X @ self.Vk @ self.beta + self.model.intercept_\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCR with expanding window\n",
    "def EXP_PCR(data,fts,tgt,yrs_trn,yrs_vld,params):\n",
    "    X = data[fts]\n",
    "    y = data[[tgt]]\n",
    "    y_pred = []\n",
    "\n",
    "    # deal with dates\n",
    "    all_dates = pd.Series(data.index).drop_duplicates().sort_values().reset_index(drop=True)\n",
    "\n",
    "    # train validate test split and prediction\n",
    "    for i in np.arange(12*yrs_trn-1,len(all_dates[:-12*(yrs_vld+1)]),12):\n",
    "        nddt_trn = all_dates[i]\n",
    "        nddt_vld = all_dates[i+yrs_vld*12]\n",
    "        nddt_tst = all_dates[i+(yrs_vld+1)*12]\n",
    "        X_trn, y_trn = X[X.index<=nddt_trn], y[y.index<=nddt_trn]\n",
    "        X_vld, y_vld = X[(X.index>nddt_trn)&(X.index<=nddt_vld)], y[(y.index>nddt_trn)&(y.index<=nddt_vld)]\n",
    "        X_tst = X[(X.index>nddt_vld)&(X.index<=nddt_tst)]\n",
    "        mod = val_fun(PCRegressor, params, X_trn, y_trn, X_vld, y_vld)\n",
    "        y_pred += list(mod.predict(X_tst).flatten())\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENet with expanding window\n",
    "def EXP_ENet(data,fts,tgt,yrs_trn,yrs_vld,params):\n",
    "    X = data[fts]\n",
    "    y = data[[tgt]]\n",
    "    y_pred = []\n",
    "\n",
    "    # deal with dates\n",
    "    all_dates = pd.Series(data.index).drop_duplicates().sort_values().reset_index(drop=True)\n",
    "\n",
    "    # train validate test split and prediction\n",
    "    for i in np.arange(12*yrs_trn-1,len(all_dates[:-12*(yrs_vld+1)]),12):\n",
    "        nddt_trn = all_dates[i]\n",
    "        nddt_vld = all_dates[i+yrs_vld*12]\n",
    "        nddt_tst = all_dates[i+(yrs_vld+1)*12]\n",
    "        X_trn, y_trn = X[X.index<=nddt_trn], y[y.index<=nddt_trn]\n",
    "        X_vld, y_vld = X[(X.index>nddt_trn)&(X.index<=nddt_vld)], y[(y.index>nddt_trn)&(y.index<=nddt_vld)]\n",
    "        X_tst = X[(X.index>nddt_vld)&(X.index<=nddt_tst)]\n",
    "        mod = val_fun(ElasticNet, params, X_trn, y_trn, X_vld, y_vld)\n",
    "        y_pred += list(mod.predict(X_tst).flatten())\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Run\")\n",
    "stdt, nddt = 20010131, 20201231\n",
    "yrs_trn, yrs_vld = 6, 4\n",
    "stdt_tst = str(stdt + (yrs_trn+yrs_vld)*10000)\n",
    "stdt_tst = np.datetime64(stdt_tst[:4]+'-'+stdt_tst[4:6]+'-'+stdt_tst[6:])\n",
    "\n",
    "file_path = './data/'\n",
    "filename_ch = 'GKX_20201231.csv' # filename of firm characteristics data\n",
    "filename_ma = 'PredictorData2022_Monthly.csv' # filename of macroeconomic predictors data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\INHA\\Desktop\\Empirical_Asset_Pricing_via_Machine_Learning\\My\\Gu_et_al_2020.ipynb Cell 22\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/INHA/Desktop/Empirical_Asset_Pricing_via_Machine_Learning/My/Gu_et_al_2020.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m st \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/INHA/Desktop/Empirical_Asset_Pricing_via_Machine_Learning/My/Gu_et_al_2020.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m## firm characteristics data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/INHA/Desktop/Empirical_Asset_Pricing_via_Machine_Learning/My/Gu_et_al_2020.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data_ch \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file_path\u001b[39m+\u001b[39mfilename_ch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/INHA/Desktop/Empirical_Asset_Pricing_via_Machine_Learning/My/Gu_et_al_2020.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# pick out top and bottom 1000 firms\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/INHA/Desktop/Empirical_Asset_Pricing_via_Machine_Learning/My/Gu_et_al_2020.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data_ch_top \u001b[39m=\u001b[39m data_ch\u001b[39m.\u001b[39msort_values(\u001b[39m'\u001b[39m\u001b[39mmvel1\u001b[39m\u001b[39m'\u001b[39m,ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mDATE\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mhead(\u001b[39m1000\u001b[39m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file_path' is not defined"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "## firm characteristics data\n",
    "data_ch = pd.read_csv(file_path+filename_ch)\n",
    "# pick out top and bottom 1000 firms\n",
    "data_ch_top = data_ch.sort_values('mvel1',ascending=False).groupby('DATE').head(1000).reset_index(drop=True)\n",
    "data_ch_bot = data_ch.sort_values('mvel1',ascending=False).groupby('DATE').tail(1000).reset_index(drop=True)\n",
    "\n",
    "## macroeconomic predictors data\n",
    "data_ma = pd.read_csv(file_path+filename_ma)\n",
    "\n",
    "## data preprocessing\n",
    "data = data_preprocessing(data_ch,data_ma,stdt,nddt)\n",
    "data_top = data_preprocessing(data_ch_top,data_ma,stdt,nddt)\n",
    "data_bot = data_preprocessing(data_ch_bot,data_ma,stdt,nddt)\n",
    "# features in each dataset\n",
    "features = list(set(data.columns).difference({'permno','DATE','RET'}))\n",
    "features_top = list(set(data_top.columns).difference({'permno','DATE','RET'}))\n",
    "features_bot = list(set(data_bot.columns).difference({'permno','DATE','RET'}))\n",
    "elapsed_time = time.time() - st\n",
    "print(f'Data preprocessed!!!!!\\n Execution time: {time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## df to store true and pred RET\n",
    "#res = data[['permno','RET']][data.index>=stdt_tst].copy()\n",
    "res_top = data_top[['permno','RET']][data_top.index>=stdt_tst].copy()\n",
    "#res_bot = data_bot[['permno','RET']][data_bot.index>=stdt_tst].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear Models ###\n",
    "## OLS\n",
    "st = time.time()\n",
    "#res['OLS_PRED'] = EXP_OLS(data,features,'RET',yrs_trn+yrs_vld)\n",
    "res_top['OLS_PRED'], res_top_roos_OLS= EXP_OLS(data_top,features_top,'RET',yrs_trn+yrs_vld)\n",
    "#res_bot['OLS_PRED'] = EXP_OLS(data_bot,features_bot,'RET',yrs_trn+yrs_vld)\n",
    "#save_pred(file_path,res,res_top,res_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def calculate_variable_importance(data, fts, tgt, yrs_trn, model):\n",
    "    # Variable importance storage\n",
    "    X = data[fts]\n",
    "    y = data[[tgt]]\n",
    "    R2_oos_all = []\n",
    "    var_imp_dict = {}\n",
    "\n",
    "    # deal with dates\n",
    "    all_dates = pd.Series(data.index).drop_duplicates().sort_values().reset_index(drop=True)\n",
    "\n",
    "    def compute_importance(i):\n",
    "        nddt_trn = all_dates[i]\n",
    "        nddt_tst = all_dates[i+12]\n",
    "        X_trn, y_trn = X[X.index<=nddt_trn], y[y.index<=nddt_trn]\n",
    "        X_tst, y_tst= X[(X.index>nddt_trn)&(X.index<=nddt_tst)], y[(y.index>nddt_trn)&(y.index<=nddt_tst)]\n",
    "\n",
    "        mod = model.fit(X_trn, y_trn)\n",
    "        y_pred = mod.predict(X_tst)\n",
    "        R2_oos_all.append(R_oos(y_tst, y_pred))\n",
    "\n",
    "        importance_dict = {\"All Features\": R2_oos_all}\n",
    "\n",
    "        # Assess importance of each feature\n",
    "        for feature in fts:\n",
    "            X_trn_drop = X_trn.drop(feature, axis=1).copy()\n",
    "            X_tst_drop = X_tst.drop(feature, axis=1).copy()\n",
    "\n",
    "            mod_drop = LinearRegression().fit(X_trn_drop, y_trn)\n",
    "            y_pred_drop = mod_drop.predict(X_tst_drop)\n",
    "\n",
    "            R2_oos_drop = R_oos(y_tst, y_pred_drop)\n",
    "            importance_dict[feature] = R2_oos_all - R2_oos_drop\n",
    "            \n",
    "            var_imp_dict[f\"{features}\"+f\"{nddt_tst}\"] = importance_dict\n",
    "        print(f\"{nddt_tst}Variable Importance Computation Complete\")\n",
    "\n",
    "    # Parallelize the for loop\n",
    "    Parallel(n_jobs=-1)(delayed(compute_importance)(i) for i in np.arange(12*yrs_trn-1,len(all_dates[:-12]),12))\n",
    "\n",
    "    return var_imp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonHome_p36",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
